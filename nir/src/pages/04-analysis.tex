\section{Анализ предметной области}

\subsection{Распределенные системы}

За прошедшие годы было представлено несколько определений распределенных систем, но ни одно из них не является удовлетворительными и не согласуются друг с другом. 
Среди некоторых определений Кулуриса~\cite{2009distributed} есть одно, которое определяет распределенную систему как <<систему, в которой аппаратные или программные компоненты, расположенные на подключенных к сети компьютерах, взаимодействуют и координируют свои действия только путем передачи сообщений>>.

Еще одно определение, дающее общее представление о распределенных cистемах звучит следующим образом:

Распределенная система --- это вычислительная среда, в которой многочисленные компоненты расположены на нескольких вычислительных устройствах в сети. 
Эти устройства разделяют работу, обмениваясь данными и координируя свои усилия, чтобы конечному пользователю казаться единой сплоченной системой. 
Распределенная система может представлять собой совокупность различных конфигураций, таких как мэйнфреймы, компьютеры, рабочие станции и миникомпьютеры~\cite{khole2023compendium}.

Любое централизованное решение основано на том, что один узел назначается ответственным за все вычисления проводимые приложением, обрабатывает их локально, и данный узел является общим для всех пользователей системы.
Следовательно, существует единая точка контроля и единая точка отказа. 
Возможно такого рода решение и является оптимальным для небольших задач, но при увеличении нагрузки на систему, возникает, например, необходимость в более отказоустойчивой системе, которая будет обеспечивать доступность данных на протяжении всего времени работы системы.

На рис.~\ref{img:nodes} показаны различные архитектурные подходы к построению вычислительных систем.

\img{70mm}{nodes}{Обзор архитектур вычислительных систем}

Мотивацией роста децентрализованных вычислений является доступность недорогих, высокопроизводительных компьютеров и сетевых инструментов. 
Такая система может обладать более высокой производительностью, чем один конкретный суперкомпьютер. 
Целью таких систем является минимизация затрат на связь и вычисления. 
В распределенных системах этапы обработки приложения распределены между участвующими в ней узлами. 
Основным шагом во всех архитектурах распределенных вычислений является понятие связи между узлами системы.

На рис.~\ref{img:dist} представлено сравнение нескольких наиболее важных распределенных систем с тремя суперкомпьютерами, включенными в список \texttt{TOP500} \cite{top500}. 
Что касается общей производительности, распределенные системы часто ненамного отстают от технологий, используемых в настоящее время \texttt{NASA} или военными, для функционирования которых требуется значительный финансовый капитал. 
Для создания диаграммы для суперкомпьютеров были использованы результаты производительности \texttt{LINPACK}~\cite{linpack}, которые измеряют скорость решения сложной системы линейных уравнений. 
С другой стороны, для распределенных систем была собрана информация, предоставленная системными администраторами.
Чаще всего они основаны на количестве загруженных результатов за определенный промежуток времени.

\img{90mm}{dist}{Сравнение производительности различных вычислительных систем.}

Приложение, утилизирующее распределенные вычисления, обычно строится из стандартных <<блоков>>, которые предоставляют необходимую функциональность.
Так например, приложение может требовать реализации следующих функций:
\begin{itemize}
  \item[$-$] хранение данных для дальнейшего к ним доступа (базы данных);
  \item[$-$] хранение результата дорогостоящей операции для ускорения чтения (кэширование);
  \item[$-$] возможность для пользователей искать данные по ключевому слову или фильтровать их различными способами (поисковые индексы);
  \item[$-$] отправление сообщений другому процессу, которые будут обрабатываться асинхронно (потоковая обработка);
  \item[$-$] периодическая обработка большого объема накопленных данных (пакетная обработка).
\end{itemize}

На рис.~\ref{img:distr_arch} представлена общая организация абстрактной распределенной системы.

\img{90mm}{distr_arch}{Организация распределенной системы.}

Как и к любому другому ПО, к распределенным системам применимы часто рассматриваемые инженерные критерии~\cite{kleppmann2017designing}, а именно:
\begin{enumerate}
  \item \texttt{Reliability} --- Надежность --- Система должна продолжать корректно работать (выдавать корректный результат на
желаемом уровне производительности) даже перед лицом неблагоприятных факторов (аппаратных или
программных сбоев и возможной человеческой ошибки); 
  \item \texttt{Scalability} --- Масштабируемость --- По мере роста системы (при увеличении объема данных, трафика или сложности) должны существовать разумные способы и инструменты управления этим ростом;
  \item \texttt{Maintainability} --- Сопровождаемость --- Со временем, набор разработчиков, работающих над системой, может сильно изменяться, и все они должны быть в состоянии продуктивно работать над продуктом (проектирование и эксплуатация, как поддержание текущего поведения, так и адаптация системы к новым требованиям). 
\end{enumerate}
% В дальнейшем описываемые алгоритмы будут рассматриваться через призму этих критериев: как результирующие системы удовлетворяют этим критериям и какие компромиссы приходится делать при их реализации.
%
% Также по мере разработки распределенных систем, существует набор различных вопросов, на которые необходимо дать ответ для корректной и эффективной работы системы как таковой.
% Например, как распределить задачи между узлами системы и как обеспечить надежность и целостность данных, 
% как гарантировать доступность данных на всем протяжении работы системы и т.д.
% Одним из формальных ответов на эти вопросы служит формулировка \texttt{CAP}-теоремы --- или теорема Брюэра --- которая звучит следующим образом~\cite{kleppmann2017designing}: 
%
% В любой распределенной системе можно обеспечить не более двух из трех свойств:
% \begin{itemize}
%   \item[$-$] \textbf{C}onsistency --- Согласованность --- каждый сервер возвращает верный ответ на каждый запрос, т.е. ответ, который является правильным в соответствии с требуемой спецификацией приложения;
%   \item[$-$] \textbf{A}vailability --- Доступность --- гарантируется, что на каждый запрос в конечном итоге будет получен некий ответ;
%   \item[$-$] \textbf{P}artition tolerance --- Устойчивость к разделению --- в произвольный момент времени сервера могут быть разделены на несколько групп, которые не будут взаимодействовать друг с другом. Иными словами, сообщения в такой системе могут задерживаться, а иногда и теряться.
% \end{itemize}
%
% К сожалению, такая формулировка вводит в заблуждение~\cite{cap-changed}, поскольку в случае если система существует в сети, которая может терять произвольное количество сообщений, что является реальностью современных телекоммуникаций, то такая система не может быть одновременно доступной и согласованной, выбрать следует что-то одно~\cite{cap-confusion}.
% Поэтому в дальнейшем, при рассмотрении распределенных систем, будут браться во внимание именно эти два критерия: доступность и согласованность, принимая устойчивость к разделению как данность.
% Это необходимое допущение позволяет лучше понять проблематику распределенных вычислений и рассматривать их в контексте реальных систем.

\subsection{Базовые понятия}

\subsubsection{Пакетная обработка}

В производственных условиях пакетная обработка определяется как одновременная обработка нескольких обращений~\cite{pinedo2012scheduling}. 
То же самое может относиться к процессам обслуживания, например, некоему информационному сеансу, который организуется для группы клиентов, а не индивидуально. 
Однако Батист~\cite{Baptiste2000} утверждает, что продолжительность обработки пакета также может быть определена суммой времени обработки всех отдельных обращений. 
Это намекает на форму пакетной обработки, при которой обращения обрабатываются последовательно. 

Из предыдущего следует, что можно различать различные типы пакетной обработки. 
В целом, пакетная обработка определяется как одновременное, (квази-)последовательное или параллельное выполнение действия в различных случаях одним и тем же ресурсом~\cite{Martin2015BatchPD}. 
Следовательно, рассматриваются три типа пакетной обработки, которые проиллюстрированы на примере на рис.~\ref{img:batch}, где действие всегда выполняется одним и тем же ресурсом в обоих случаях.

\begin{itemize}
  \item Одновременная пакетная обработка. 

    Объекты действия находятся в одном пакете, когда они выполняются одним и тем же ресурсом для разных случаев в одно и то же время. 
    Например: несколько деталей автомобиля, которые необходимо покрасить в один и тот же цвет, можно собрать вместе в покрасочной камере. 
    На рисунке~\ref{img:batch} два объекта типа \texttt{B} представляют собой одновременный пакет, поскольку время запуска и завершения в разных случаях соответствует.
  \item Последовательная пакетная обработка. 

    Объекты действия находятся в последовательном пакете, когда они выполняются одним и тем же ресурсом для разных случаев сразу или почти сразу друг за другом. 
    Например: сотрудники обрабатывают свои электронные письма только два раза в день, когда они обрабатываются последовательно, где может наблюдаться задержка в несколько секунд между обработкой двух разных электронных писем. 
    Два объекта типа \texttt{A} на рисунке~\ref{img:batch} образуют последовательный пакет, поскольку начальная временная метка для второго случая соответствует конечной временной метке для первого случая.
  \item Параллельная пакетная обработка. 

    Объекты действия находятся в параллельном пакете, когда они выполняются одним и тем же ресурсом для отдельных случаев, частично перекрывающихся по времени. 
    Например: клерк уже может начать бронирование второго счета, когда требуется дополнительная информация для завершения первого. 
    На рисунке~\ref{img:batch} объекты \texttt{C, D, E, F, G} представляют типы параллельных пакетов.
\end{itemize}

\img{17mm}{batch}{Схематическое представление двух случаев, в которых действие всегда выполняется одним и тем же ресурсом для обоих случаев.}

\subsubsection{Потоковая обработка}

Поток данных, используемый при потоковой обработке данных, представляет собой счетную бесконечную последовательность элементов и используется для представления элементов данных, которые становятся доступными с течением времени. 
Примерами являются показания датчиков в приложении для мониторинга окружающей среды, котировки акций в финансовых приложениях или сетевые данные в приложениях для компьютерного мониторинга. 
Система с потоковой обработкой данных анализирует элементы из потоков по мере их поступления, чтобы своевременно выдавать новые результаты и при необходимости быстро реагировать на эти данные.

\clearpage % подгоняем

\textbf{Модели}

Потоки могут быть структурированными или неструктурированными. 
В структурированном потоке элементы соответствуют определенному формату или схеме, что позволяет дополнительно моделировать поток. 

Напротив, неструктурированные потоки могут иметь произвольное содержимое, часто являющееся результатом объединения потоков из многих источников~\cite{10.1007/978-3-642-53974-9_6}.

Существуют три основные модели структурированных потоков, которые различаются по тому, как элементы потока связаны друг с другом и влияют друг на друга~\cite{stream-def}: 
\begin{enumerate}
  \item модель турникета;
  \item модель кассового аппарата; 
  \item модель временных рядов. 
\end{enumerate}

Наиболее общей моделью является модель турникета. 
В этой модели поток моделируется как вектор элементов, и каждый элемент $S_{i}$ в потоке представляет собой модификацию (увеличение или уменьшение) элемента базового вектора. 
Размер вектора в этой модели является всей областью элементов потока. 
Эта модель также является моделью, обычно используемой в традиционных системах баз данных, для которой определены \texttt{CRUD} операциии. 

В модели кассового аппарата элементы в потоке являются только дополнениями к базовому вектору, и никогда больше не смогут покинуть этот вектор. 
Это похоже на базы данных, записывающие историю отношений. 

Наконец, модель временных рядов рассматривает каждый элемент в потоке $S_{i}$ как новую независимую векторную запись. 
В результате базовая модель представляет собой постоянно увеличивающийся вектор и, как правило, неограниченный. 
Поскольку в этой модели каждый элемент может обрабатываться индивидуально, он часто используется в современных механизмах потоковой обработки.

\clearpage % подгоняем

\textbf{Время}

Время является центральным понятием во многих системах для обработки потоков, либо потому, что они заинтересованы в обновлении своего взгляда на мир с учетом последних данных, полученных из потоков, либо потому, что они нацелены на выявление временных тенденций во входных потоках~\cite{stream-def}. 

По этой причине в большинстве моделей потоковой обработки элементы данных связаны с некоторой временной меткой из заданной временной области. 
Общая семантика времени включает время события, которое является временем создания элемента, и время обработки, которое является временем, когда система потоковой обработки начинает обрабатывать элементы~\cite{10.14778/2824032.2824076}.

Различная семантика времени создает различные проблемы с точки зрения порядка и синхронизации. 
Интуитивно понятно, что, хотя время события и время обработки в идеале должны быть равны, на практике несинхронизированные часы систем, а также переменные задержки связи и обработки приводят к перекосу между временем события и временем обработки, который не только отличен от нуля, но и сильно варьируется~\cite{10.14778/2824032.2824076}.

\textbf{Окна}

Окна являются одной из основных частей практически всех систем потоковой обработки. 
Они определяют ограниченное количество элементов в неограниченном потоке данных. 
Они используются для выполнения вычислений, которые были бы невозможны (бесконечны) в случае неограниченных данных, таких как вычисление среднего значения всех элементов в потоке чисел. 
Наиболее распространенными типами окон являются окна, основанные на подсчете и времени. 
Первые определяют свой размер в терминах количества элементов, которые они включают, в то время как вторые определяют свой размер в терминах временного интервала и включают все элементы с временной меткой, включенные в этот временной интервал. 

В обоих случаях мы различаем плавающие окна, которые непрерывно перемещаются с появлением новых элементов, таким образом, всегда захватывая новые элементы, и сменяющиеся окна, которые могут накапливать несколько элементов перед перемещением~\cite{10.14778/1920841.1920874}.

Совсем недавно были определены новые типы окон, чтобы лучше отражать потребности приложений. 
Они включают в себя окна, определяемые сеансом и данными, которые изменяются по размеру и определяют свои границы на основе элементов данных: 
например, в приложении для мониторинга программного обеспечения окно может включать все и только те элементы, которые относятся к сеансу, открытому конкретным пользователем этого программного обеспечения~\cite{10.14778/2824032.2824076}.

\subsubsection{Алгоритм выбора лидера}

В распределенных алгоритмах выборы лидера --- это процесс назначения одного процесса организатором, координатором, инициатором или последовательностью выполнения некоторой задачи, распределенной между несколькими компьютерами (узлами).
Другими словами, выбор лидера --- это процесс определения того, что процесс будет управлять некоторой задачей, распределенной между несколькими процессами или узлами~\cite{leader-krchowdharry}.

Ниже приведены причины выбора лидера:
\begin{enumerate}
  \item Централизованное управление упрощает синхронизацию процессов, но в таком случае это приведет к появлению единой точки отказа системы.
  \item Предоставить решение для выбора нового управляющего узла (лидера) при отказе существующего лидера.
  \item Существует множество алгоритмов выбора лидера, и один из них может быть выбран на основе конкретных требований. Примерами таких алгоритмов являются алгоритм забияки или алгоритм Чанга и Робертса
\end{enumerate}

Существует два критерия отбора:
\begin{itemize}
  \item Поиск экстремумов: основан на глобальном приоритете. Каждый процесс характеризуется фиксированным значением оценки.
  \item Основанный на предпочтениях: Процессы в группе могут голосовать за лидера на основе личных предпочтений.
\end{itemize}

\clearpage % подгоняем

\subsection*{Вывод}

В данном разделе были рассмотрены основные понятия, связанные с распределенными вычислениями, и представлены основные алгоритмы, используемые в таких системах.
Также были рассмотрены причины использования распределенных систем и сравнение их производительности с суперкомпьютерами.
Таким образом, в данном разделе была представлена общая картина распределенных вычислений, которая позволяет понять проблематику, связанную с этой областью, и рассматривать их в контексте реальных систем.

\section{Существующие решения}
\subsection{Hadoop MapReduce}

\texttt{MapReduce} --- это программная модель для обработки генерации больших наборов данныхи и связанная с ней реализация в виде системы \texttt{Hadoop}.
Пользователи задают функцию сопоставления (\texttt{Map}), которая обрабатывает некоторую пару ключ/значения для генерации набора промежуточных пар ключ/значение, и функцию уменьшения (\texttt{Reduce}), которая объединяет все промежуточные значения, связанные с одним и тем же промежуточным ключом. 
Программы, написанные в этом функциональном стиле, автоматически распараллеливаются и выполняются на большом кластере обычных машин. 
Система выполнения заботится о разделении входных данных, планирования выполнения программы на множестве машин, обработки сбоев машин и управления необходимой межмашинной связью~\cite{google-mapreduce}.
\subsubsection{Программная модель}

Вычисления в \texttt{MapReduce} используют набор входных пар ключ/значение и создает набор выходных пар ключ/значение. 
Пользователь библиотеки на основе \texttt{MapReduce} выражает вычисления в виде двух функций: \texttt{Map} и \texttt{Reduce}.

Функция \texttt{Map}, написанная пользователем, принимает входную пару и создает набор промежуточных пар ключ/значение. 
Алгоритм \texttt{MapReduce} группирует все промежуточные значения, связанные с одним и тем же промежуточным ключом \texttt{I}, и передает их функции \texttt{Reduce}.

Функция \texttt{Reduce}, также написанная пользователем, принимает промежуточный ключ \texttt{I} и набор значений для этого ключа. 
Оно объединяет эти значения для формирования возможно меньшего набора значений. 
Обычно при каждом вызове \texttt{Reduce} создается только нулевое или одно выходное значение. 
Промежуточные значения передаются в пользовательскую функцию \texttt{Reduce} через итератор.
Это позволяет нам обрабатывать списки значений, которые слишком велики, чтобы поместиться в памяти.

\subsubsection{Реализация}
В качестве примера реализации алгоритма \texttt{MapReduce} была взята имплементация системы \texttt{Hadoop}, разработанная в рамках \texttt{Apache Foundation}, основанная на публицкации сотрудников компании \texttt{Google}~\cite{google-mapreduce}.
На рис.~\ref{img:map_reduce} изображена общая схема алгоритма.%, описанная инженерами из \texttt{Google}.

\img{90mm}{map_reduce}{Схема работы алгоритма \texttt{MapReduce}}

Когда пользовательская программа вызывает функцию \texttt{MapReduce}, выполняется следующая последовательность действий (нумерованные метки на рис.~\ref{img:map_reduce} соответствуют номерам в списке ниже):

\begin{enumerate}
  \item Алгоритм разбивает входные файлы на $M$ фрагментов, которые затем запускает множество копий программы на кластере компьютеров.
  \item Одна из копий программы является специальной --- \texttt{master}. 
    Остальные --- это процессы, которым \texttt{master} назначает работу. 
    В общем итоге назначается $M$ задач типа \texttt{Map} и $R$ задач типа \texttt{Reduce}.
  \item Процесс, которому назначена задача типа \texttt{Map}, считывает содержимое соответствующего фрагмента данных.
    Происходит анализ пары ключ/значение из входных данных и передает каждую пару в определенную пользователем функцию \texttt{Reduce}. 
    Промежуточные пары ключ/значение, созданные функцией \texttt{Reduce} буферизуются в памяти.
  \item Буферизованные пары записываются на локальный диск, разбитые на $R$ фрагментов с помощью функции секционирования.
    Расположение этих буферизованных пар на локальном диске передается обратно ведущему процессу, который отвечает за пересылку этих местоположений процессам с назначенными функциями \texttt{Reduce}.
  \item Когда ведущий процесс уведомляет рабочий процесс с назначенной функцией \texttt{Reduce} об этих местоположениях, он использует удаленные вызовы процедур для считывания буферизованных данных с локальных дисков рабочих процессов типа \texttt{Map}.
    Когда рабочий процесс типа \texttt{Reduce} прочитал все промежуточные данные, он сортирует их по промежуточным ключам, чтобы все вхождения одного и того же ключа были сгруппированы вместе. 
    Сортировка необходима, потому что обычно много разных ключей сопоставляются с одной и той же задачей \texttt{Reduce}. 
  \item Рабочий процесс типа \texttt{Reduce} выполняет итерацию по отсортированным промежуточным данным и для каждого встреченного уникального промежуточного ключа передает ключ и соответствующий набор промежуточных значений пользовательской функции \texttt{Reduce}.
    Выходные данные функции \texttt{Reduce} добавляются в конечный выходной файл для этого раздела \texttt{Reduce}.
  \item Когда все задачи \texttt{Map} и \texttt{Reduce} завершены, ведущий процесс запускает пользовательскую программу. 
    На этом этапе вызов \texttt{MapReduce} в пользовательской программе возвращается обратно к пользовательскому коду.
\end{enumerate}

После успешного завершения \texttt{MapReduce} выходные данные помещаются в $R$ выходных файлах (по одному для каждой задачи \texttt{Reduce}, с именами файлов, указанными пользователем).
Как правило, пользователям не нужно объединять эти выходные файлы $R$ в один файл --- зачастую эти файлы передаются в качестве входных данных другому вызову \texttt{MapReduce} или используют их в другом распределенном приложения, способного обрабатывать входные данные, разделенные на несколько файлов.

\subsubsection{Экосистема Hadoop}
 
Различные проекты экосистемы \texttt{Apache Hadoop} взаимосвязаны, как показано на рисунке~\ref{img:hadoop-eco}.

\img{110mm}{hadoop-eco}{Экосистема \texttt{Apache Hadoop}}

\texttt{MapReduce} обрабатывает данные, хранящиеся в \texttt{HDFS}. 
\texttt{HBase} --- это \texttt{NoSQL} база данных, которая поддерживает все виды данных и, следовательно, способна обрабатывать все, что есть в базе данных \texttt{Hadoop}. 
\texttt{Hive} --- система управления базами данных с \texttt{SQL}-подобным языком запросов, позволяет выполнять запросы, агрегировать и анализировать данные системы.
По умолчанию \texttt{HBase} и \texttt{Hive} хранят данные в \texttt{HDFS}. 
\texttt{Sqoop} использоуется для массовой передачи данных из реляционной системы управления базами данных (РСУБД) в \texttt{HDFS}, \texttt{Hive} и \texttt{HBase}. 
\texttt{Sqoop} также поддерживает массовую передачу данных из \texttt{HDFS} в реляционную базу данных. 
\texttt{Flume}  используется для потоковой передачи данных журнала, приемником которой может быть \texttt{HDFS}, \texttt{Hive}, \texttt{HBase} или \texttt{Solr}.
\texttt{Flume}, основанный на источниках и приемниках, поддерживает несколько видов источников и приемников с акцентом на потоковую передачу данных в режиме реального времени в отличие от одноразовой массовой передачи с помощью \texttt{Sqoop}. 
\texttt{Solr} используется для индексации данных из \texttt{HDFS}, \texttt{Hive}, \texttt{HBase} и СУБД. 
\texttt{HDFS} хранит данные в дисковой файловой системе и фактически является абстрактной файловой системой поверх дисковой файловой системы. 
\texttt{Solr} также хранит данные в дисковой файловой системе по умолчанию, но также может хранить индексированные данные в \texttt{HDFS}~\cite{vohra2016practical}.

Помимо вышеперечисленных компонентов, есть и некоторые другие, которые выполняют задачу сделать \texttt{Hadoop} способным обрабатывать большие наборы данных:
\begin{itemize}
  \item \texttt{YARN} --- подсистема, помогающая управлять ресурсами в кластерах, состоящя из 3 компонентов: менеджер ресурсов, менеджер узлов и менеджер приложений~\cite{Perwej2017AnEE}. 
    \texttt{YARN} выполняет планирование и распределение ресурсов для системы \texttt{Hadoop}.
    Менеджер ресурсов имеет привилегию выделять ресурсы для приложений в системе, в то время как менеджер узлов работает над распределением ресурсов, таких как процессорное, память, пропускная способность, а затем подтверждают это менеджеру ресурсов.
    Менеджер приложений работает как интерфейс между менеджером ресурсов и менеджером узлов и выполняет согласование в соответствии с требованиями обоих.
  \item \texttt{Apache ZooKeeper} --- открытая программная служба для координации распределённых систем, организованная на основе резидентной базы данных категории <<ключ --- значение>>. 
    Изначально входила в экосистему \texttt{Hadoop}, впоследствии стала проектом верхнего уровня \texttt{Apache Software Foundation}
  \item \texttt{Pig} --- платформа для структурирования потока данных, обработки и анализа огромных массивов данных.
    \texttt{Pig} выполняет работу по выполнению команд, где в фоновом режиме выполняются все действия \texttt{MapReduce}. 
    После обработки \texttt{Pig} сохраняет результат в \texttt{HDFS}. Язык \texttt{Pig Latin} специально разработан для этого фреймворка, который работает в собственной среде выполнения \texttt{Pig Runtime}. 
\end{itemize}

% Некоторые элементы могут сильно отличаться друг от друга с точки зрения их архитектуры; однако их функциональные возможности исходят из масштабируемости и мощности \texttt{Hadoop}. 
% Экосистема Hadoop разделена на четыре различных уровня: хранение данных, обработка данных, доступ к данным, управление данными. 
% На рисунке~\ref{hadoop-eco-over} показано, как различные элементы \texttt{Hadoop} задействованы на различных уровнях обработки данных.

\subsection{Apache Spark и RDD}

\subsubsection{Программная модель}

\texttt{Apache Spark} --- это унифицированный аналитический движок для крупномасштабной обработки данных~\cite{apache-spark}.
Основной абстракцией в \texttt{Spark} является устойчивый распределенный набор данных (Resilient Distribute Dataset -- RDD), который представляет собой доступную только для чтения коллекцию объектов, распределенных по набору машин, которые могут быть восстановлены в случае потери раздела.

Чтобы использовать \texttt{Spark}, разработчики пишут программу-драйвер, которая реализует высокоуровневый поток управления их приложением и параллельно запускает различные операции. 
\texttt{Spark} предоставляет две основные абстракции для распределения вычислений: устойчивые распределенные наборы данных и параллельные операции с этими наборами данных (вызываемые путем передачи функции для применения к набору данных).
Кроме того, \texttt{Spark} поддерживает два ограниченных типа общих переменных, которые могут использоваться в функциях, запущенных в кластере~\cite{180560}.

\textbf{RDD}

Формально \texttt{RDD} --- это доступная только для чтения секционированная коллекция записей. 
\texttt{RDD} могут быть созданы только с помощью детерминированных операций либо с данными в некотором стабильном хранилище, либо с другими \texttt{RDD}. 
Данные операции называются преобразованиями, чтобы отличать их от других операций с \texttt{RDD}. 
Примеры преобразований включают такие операции как отображение ($map$), фильтрацию ($filter$) и объединение ($join$).

Элементы \texttt{RDD} необязательно должны существовать в физическом хранилище; 
вместо этого дескриптор \texttt{RDD} содержит достаточно информации для вычисления \texttt{RDD}, начиная с данных в хранилище. 
Это означает, что \texttt{RDD} всегда можно восстановить в случае сбоя узлов. 

В \texttt{Spark} каждый \texttt{RDD} представлен объектом \texttt{Scala}.
\texttt{Spark} позволяет программистам создавать \texttt{RDD} четырьмя способами:

\begin{itemize}
  \item Из файла в общей файловой системе.
  \item Путем <<распараллеливания>> коллекции \texttt{Scala} (например, массива) в программе драйвера, что означает разделение ее на несколько фрагментов, которые будут отправлены нескольким узлам.
  \item Путем преобразования существующего \texttt{RDD}. 
    Набор данных с элементами типа $A$ может быть преобразован в набор данных с элементами типа $B$ с помощью операции, называемой \texttt{flatMap}, которая передает каждый элемент через предоставленный пользователем функция типа $A \rightarrow List[B]$. 
    Другие преобразования могут быть выражены с помощью \texttt{flatMap}, включая \texttt{Map} (передача элементов через функцию типа $A \rightarrow B$) и \texttt{Filter} (выбор элементов, соответствующих предикату).
    
    Стоит отметить, что \texttt{flatMap} имеет ту же семантику, что \texttt{Map} в \texttt{MapReduce}.
  \item Путем изменения времени жизни существующего \texttt{RDD}.
    По умолчанию \texttt{RDD} являются ленивыми и эфемерными. 
    То есть разделы набора данных материализуются по требованию, когда они используются в параллельной операции (например, путем передачи блока файла через функцию \texttt{Map}), и удаляются из памяти после использования. 
    Однако пользователь может изменить время жизни \texttt{RDD} с помощью двух действий:
    \begin{enumerate}
      \item Кэширование. 
        Данное действие оставляет \texttt{RDD} ленивым, но намекает на то, что его следует сохранить в памяти после первого вычисления, потому что он будет использован повторно.
      \item Сохранить. 
        Данное действие оценивает набор данных и записывает его в распределенную файловую систему. 
        Сохраненная версия используется в будущих операциях.
    \end{enumerate}
\end{itemize}

\textbf{Параллельные операции}

Ниже перечислены основные преобразования \texttt{RDD} и действия, доступные в \texttt{Spark}. 
Следует отметить, что преобразования --- это отложенные (ленивые) операции, которые определяют новый RDD, в то время как действия запускают вычисление для возврата значения программе или записи данных во внешнее хранилище.

Трансформации: 
\begin{itemize}
  \item $map( f : T \rightarrow U) : RDD[T] \rightarrow RDD[U]$  
  \item $filter( f : T \rightarrow Bool) : RDD[T] \rightarrow RDD[T]$ 
  \item $flatMap( f : T \rightarrow Seq[U]) : RDD[T] \rightarrow RDD[U]$
  \item $sample(fraction : Float) : RDD[T] \rightarrow RDD[T]$ (Детерминированная выборка) 
  \item $groupByKey() : RDD[(K, V)] \rightarrow RDD[(K, Seq[V])]$ 
  \item $reduceByKey( f : (V, V) \rightarrow V) : RDD[(K, V)] \rightarrow RDD[(K, V)]$ 
  \item $union() : (RDD[T], RDD[T]) \rightarrow RDD[T]$ 
  \item $join() : (RDD[(K, V)], RDD[(K, W)]) \rightarrow RDD[(K, (V, W))]$ 
  \item $cogroup() : (RDD[(K, V)],RDD[(K, I)]) \rightarrow RDD[(K, (Seq[V],Seq[I]))]$ 
  \item $crossProduct() : (RDD[T], RDD[U]) \rightarrow RDD[(T, U)]$ 
  \item $mapValues( f : V \rightarrow W) : RDD[(K, V)] \rightarrow RDD[(K, W)]$ (Сохраняет разделения) 
  \item $sort(c : Comparator[K]) : RDD[(K, V)] \rightarrow RDD[(K, V)]$ 
  \item $partitionBy(p : Partitioner[K]) : RDD[(K, V)] \rightarrow RDD[(K, V)]$ 
\end{itemize}

Действия:
\begin{itemize}
  \item $count() : RDD[T] \rightarrow Long$
  \item $collect() : RDD[T] \rightarrow Seq[T]$
  \item $reduce( f : (T, T) \rightarrow T) : RDD[T] \rightarrow T$
  \item $lookup(k : K) : RDD[(K, V)] \rightarrow Seq[V]$ (Воспроиводится над хэш/набор данных разделенном \texttt{RDD})
  \item $save(path : String) : $ Сохраняет RDD в файловой системе
\end{itemize}

\textbf{Разделяемые переменные}

Такие операции, как \texttt{Map}, \texttt{Filter} и \texttt{Reduce}, вызываются путем передачи функций в \texttt{Spark}.
Обычно, эти функции могут ссылаться на переменные в области, в которой они созданы.
Когда \texttt{Spark} запускает функции на рабочем узле, эти переменные в него копируются. 
Однако \texttt{Spark} также позволяет программистам создавать два ограниченных типа общих переменных для поддержки двух простых, но распространенных шаблонов использования:
\begin{itemize}
  \item Широковещательные переменные: Если большой фрагмент данных, доступный только для чтения (например, таблица поиска), используется в нескольких параллельных операциях, предпочтительнее распространять его среди рабочих только один раз, вместо того чтобы копировать его при каждом вызове функции. 
    \texttt{Spark} позволяет программисту создать такой объект, который переносит значение и гарантирует, что оно копируется каждому процессу только один раз.
  \item Накопители: Это переменные, к которым рабочие процессы могут добавлять данные только с помощью ассоциативной операции и которые может считывать только драйвер. 
    Их можно использовать для реализации счетчиков, как в \texttt{MapReduce}, и для обеспечения более императивного синтаксиса для параллельных сумм. 
    Аккумуляторы могут быть определены для любого типа, который имеет операцию <<добавить>> и значение <<ноль>>.
\end{itemize} 

\subsubsection{Реализация}

\texttt{Spark} построен поверх \texttt{Mesos}, <<кластерной операционной системы>>, которая позволяет нескольким параллельным приложениям совместно использовать кластер компьютеров и предоставляет \texttt{API} для приложений для запуска задач в этом кластере~\cite{266915}.
Это позволяет \texttt{Spark} работать совместно с существующими кластерными вычислительными платформами, такими как \texttt{Mesos}-адаптациями для \texttt{Hadoop} и \texttt{MPI}, и обмениваться с ними данными. 
Кроме того, использование \texttt{Mesos} значительно сократило затраты на программирование, которые приходилось затрачивать на \texttt{Spark}.

На рисунке~\ref{img:spark_arch} показано верхнеуровневое представление системы \texttt{Spark}.

\img{80mm}{spark_arch}{Верхнеуровневое разделение \texttt{Spark}}

Важным моментом при разработке интерфейса для взаимодействия \texttt{RDD} заключается в том, как представить зависимости между этими \texttt{RDD}.
Классифицировать такого рода зависимости можно на два типа: узкие зависимости, где каждый раздел родительского \texttt{RDD} используется не более чем одним разделом дочернего \texttt{RDD}, и широкие зависимости, где от \texttt{RDD} может зависеть несколько дочерних разделов.
Рисунок~\ref{img:spark_deps} демонстрирует примеры зависимостей \texttt{RDD} для некоторых операций.

\img{80mm}{spark_deps}{Узкие и широкие зависимости}

Всякий раз, когда пользователь запускает действие (например, $count$ или $save$) в \texttt{RDD}, планировщик проверяет линейный граф этого \texttt{RDD}, чтобы построить список этапов для выполнения, как показано на рисунке~\ref{img:spark_stages}.
Каждый этап как правило содержит наибольшее возможное количество конвейерных преобразований с узкими зависимостями.
Границами этапов являются операции перетасовки, необходимые для широких зависимостей, или любые уже вычисленные разделы, хранящиеся в памяти, которые могут прервать вычисление родительского \texttt{RDD}. 
Затем планировщик запускает задачи для вычисления недостающих разделов на каждом этапе, пока не будет вычислен целевой \texttt{RDD}.

\img{90mm}{spark_stages}{Формирование этапов в \texttt{Spark}. 
Прямоугольники со сплошными контурами --- это \texttt{RDD}.
Разделы --- это заштрихованные прямоугольники, при этом некоторые из них выделенны серым цветом, если они уже есть в памяти.
Чтобы выполнить действие в \texttt{RDD}, создаются этапы сборки с широкими зависимостями и проводятся узкие преобразования внутри каждого этапа.
В этом случае выходной \texttt{RDD} этапа 1 уже находится в оперативной памяти, поэтому есть возможность сразу запускить этап 2, а затем этап 3.}

\subsection{Pregel}

\subsubsection{Программная модель}

\texttt{Pregel} является системой для обработки большеразмерных графов, разработанной компанией \texttt{Google}~\cite{6425724}.
Архитектура \texttt{Pregel}-программ основана на объемной синхронно-параллельной модели объемной синхронно - параллельной модели Валианта~\cite{Valiant1990ABM}. 

Входными данными для вычислений в \texttt{Pregel} является ориентированный граф, в котором каждая вершина однозначно идентифицируется строковым идентификатором вершины. 
Каждая вершина связана с изменяемым значением, определяемым пользователем. 
Направленные ребра связаны со своими исходными вершинами, и каждое ребро состоит из изменяемого, определенного пользователем значения и идентификатора целевой вершины.

Вычисления в \texttt{Pregel} состоят из последовательности итераций, называемых супершагами, разделенных точками глобальной синхронизации.
Во время супершага фреймворк вызывает определенную пользователем функцию для каждой вершины, практически параллельно. 
Данная функция определяет поведение в одной вершине $V$ и на одном супершаге $S$. 
Она может считывать сообщения, отправленные в $V$ на супершаге $S − 1$, отправлять сообщения другим вершинам, которые будут получены на супершаге $S + 1$, и изменять состояние $V$ и его исходящих ребер. 
Сообщения обычно отправляются по исходящим ребрам, но оно может быть отправлено в любую вершину, идентификатор которой известен.

Завершение алгоритма основано на том, что каждая вершина графа голосует за остановку. 
На супершаге 0 каждая вершина находится в активном состоянии; все активные вершины участвуют в вычислении любого данного супершага. 
Вершина деактивирует себя, проголосовав за остановку алгоритма. 
Это означает, что вершине больше не нужно выполнять никакой работы, если она не запущена извне, и \texttt{Pregel} не будет выполнять эту вершину на последующих суперэтапах, пока не получит сообщение. 
Если вершина повторно активирована сообщением, она должна явно деактивировать себя снова. 
Алгоритм завершается, когда все вершины одновременно неактивны и нет передаваемых сообщений. 
Этот конечный автомат проиллюстрирован на рисунке~\ref{img:pregel_state}

\img{40mm}{pregel_state}{Вершинный конечный автомат.}

Выходные данные \texttt{Pregel} --- это набор значений, явно выводимых вершинами. 
Зачастую это ориентированный граф, изоморфный входным данным, но это не является необходимым свойством системы, поскольку вершины и ребра могут добавляться и удаляться во время вычислений. 
Так например, алгоритм кластеризации может генерировать небольшой набор несвязанных вершин, выбранных из большого графа. 
Алгоритм интеллектуального анализа графа может просто выводить агрегированную статистику, полученную из графа.

\subsubsection{Реализация}

\texttt{Pregel} был разработан для кластерной архитектуры \texttt{Google}.
Каждый кластер состоит из тысяч компьютеров, организованных в стойки с высокой пропускной способностью внутри стойки. 
Кластеры взаимосвязаны, но распределены географически.

\texttt{Pregel} делит граф на разделы, каждый из которых состоит из набора вершин и всех исходящих ребер этих вершин. 
Присвоение вершины разделу зависит исключительно от идентификатора вершины, что подразумевает возможность узнать, к какому разделу принадлежит данная вершина, даже если вершина принадлежит другому компьютеру или даже если вершина еще не существует. 
Функция разбиения на разделы по умолчанию --- это $hash(ID)\ mod\ N$, где $N$ -- количество разделов, которое пользователи могут изменить.

В приведенной на рисунке~\ref{img:pregel_proc} последовательности показаны четыре суперэтапа, необходимые для завершения вычисления максимального значения для графа с четырьмя узлами. 
На каждом шаге каждая вершина считывает все входящие сообщения и устанавливает максимальное значение для своего текущего значения и тех, которые были ей отправлены. 
Затем он отправляет это максимальное значение по всем своим ребрам. 
Если максимальное значение в узле не изменяется во время супершага, узел затем голосует за остановку.

\img{160mm}{pregel_proc}{Пример работы \texttt{Pregel}}

При отсутствии сбоев выполнение программы \texttt{Pregel} состоит из нескольких этапов:


\begin{enumerate}
  \item Множество копий пользовательской программы начинают выполняться на кластере машин. 
    Одна из этих копий действует как ведущая, (далее <<мастер>>). 
    <<Мастеру>> не назначается какая-либо часть графа, но он отвечает за координацию действий рабочих узлов.
    Рабочие машины используют систему управления кластером, чтобы определить местоположение <<мастер>> и отправить ей регистрационные сообщения.
  \item <<Мастер>> определяет, сколько разделов будет иметь граф, и назначает один или несколько разделов каждой рабочей машине. 
    Количество может контролироваться пользователем. 
    Наличие более одного раздела на рабочую машину обеспечивает параллелизм между разделами и лучшую балансировку нагрузки и, как правило, повышает производительность. 
    Каждый рабочий процесс отвечает за поддержание состояния своего участка графа, выполнение пользовательского метода \texttt{Compute()} в его вершинах и управление сообщениями для других рабочих узлов и от них. 
    Каждому рабочему узлу предоставляется полный набор назначений для всех узлов.
  \item <<Мастер>> назначает часть пользовательских данных каждому рабочему узлу. 
    Входные данные обрабатываются как набор записей, каждая из которых содержит произвольное количество вершин и ребер. 
    Разделение входных данных ортогонально разбиению самого графа и обычно основано на границах файла. 
    Если рабочий узел загружает вершину, принадлежащую этому рабочему участку графа, соответствующие структуры данных немедленно обновляются. 
    В противном случае данный узел помещает сообщение в очередь удаленному узлу, которому принадлежит вершина. 
    После завершения загрузки входных данных все вершины помечаются как активные.
  \item <<Мастер>> инструктирует каждый узел выполнить супершаг. 
    Рабочие узлы выполняет цикл по своим активным вершинам, используя по одному потоку для каждого раздела. 
    Узлы вызывают \texttt{Compute()} для каждой активной вершины, доставляя сообщения, которые были отправлены на предыдущем супершаге. 
    Сообщения отправляются асинхронно для обеспечения наложения вычислений, обмена данными и пакетной обработки, но доставляются сообщения до окончания супершага.
    Когда рабочий процесс завершен, он сообщает <<мастеру>>, сколько вершин будет активно на следующем суперэтапе. 
    Этот шаг повторяется до тех пор, пока активны какие-либо вершины или пока передаются какие-либо сообщения.
  \item После остановки вычислений <<мастер>> может дать указание каждому узлу сохранить свою часть графа.
\end{enumerate}

На рисунке~\ref{img:pregel_impl} изображена общая архитектура \texttt{Google Pregel}.

\img{90mm}{pregel_impl}{Архитектура \texttt{Google Pregel}}

\subsection{DryadLINQ}

\subsubsection{Программная модель}

\texttt{DryadLINQ} описывает систему для распределения вычислений \texttt{.NET LINQ} выражений на кластере \texttt{Dryad}.
Цель \texttt{DryadLINQ} --- сделать распределенные вычисления на большом вычислительном кластере достаточно простыми для каждого программиста. 
\texttt{DryadLINQ} сочетает в себе две важные части технологии \texttt{Microsoft}: механизм распределенного выполнения \texttt{Dryad} и интегрированные запросы на языке \texttt{.NET} (\texttt{LINQ})~\cite{microsoft-dryad}.

Термин \texttt{LINQ} относится к набору конструкций \texttt{.NET} для манипулирования наборами и последовательностями элементов данных.
Базовым типом для \texttt{LINQ} коллекции является тип \texttt{IEnumerable<T>}. 
Это абстрактный набор данных объектов типа $T$, доступ к которому осуществляется с помощью интерфейса итератора. 
\texttt{LINQ} также определяет интерфейс \texttt{IQueryable<T>}, который является подтипом \texttt{IEnumerable<T>} и представляет собой (неоцененное) выражение, построенное путем объединения наборов данных \texttt{LINQ} с использованием операторов \texttt{LINQ}~\cite{267963}.

\texttt{DryadLINQ} сохраняет модель программирования \texttt{LINQ} и расширяет ее для параллельного программирования, определяя небольшой набор новых операторов и типов данных.
Входные и выходные данные \texttt{DryadLINQ} вычислений представлены объектами типа \texttt{DryadTable<T>} -- подтипом \texttt{IQueryable<T>}. 
Подтипы \texttt{DryadTable<T>} поддерживают внутренних поставщиков хранилищей, которые включают в себя распределенные файловые системы, коллекции файлов \texttt{NTFS} и наборы таблиц \texttt{SQL}.

Основное ограничение, налагаемое системой \texttt{DryadLINQ} для обеспечения распределенного выполнения, заключается в том, что все функции, вызываемые в выражениях \texttt{DryadLINQ}, должны быть свободны от побочных эффектов. 
На общие объекты можно ссылаться и читать их свободно, и они будут автоматически сериализованы и распространены там, где это необходимо. 
Однако, если какой-либо общий объект изменен, результат вычисления не определен. 
\texttt{DryadLINQ} не проверяет и не обеспечивает отсутствие побочных эффектов.

\texttt{DryadLINQ} предлагает два оператора для повторного разбиения данных: \texttt{HashPartition<T,K>} и \texttt{RangePartition<T,K>}.
Эти операторы необходимы для принудительного разбиения выходного набора данных, и они также могут использоваться для переопределения выбранного оптимизатором плана выполнения. 
Однако с точки зрения \texttt{LINQ} они не являются операционными задачами, поскольку они просто реорганизуют коллекцию без изменения ее содержимого.

Оставшимися новыми операторами являются \texttt{Apply} и \texttt{Fork}, которые можно рассматривать как <<аварийный выход>>, который программист может использовать, когда требуется вычисление, которое не может быть выражено с помощью любого из встроенных операторов \texttt{LINQ}.
\texttt{Apply} принимает функцию $f$ и передает ей итератор по всей коллекции входных данных, позволяя выполнять произвольные потоковые вычисления. 
Оператор \texttt{Fork} очень похож на \texttt{Apply}, за исключением того, что он принимает один входной элемент и генерирует несколько выходных наборов данных.

Большинство программ могут быть написаны непосредственно с использованием \texttt{DryadLINQ} примитивов.
Так например, модель \texttt{MapReduce} может быть компактно изложена следующим образом в листинге~\ref{lst:dryad-map} (для ясности исключены точные сигнатуры типов):

\begin{center}
\captionsetup{justification=raggedright,singlelinecheck=off}
\begin{lstlisting}[label=lst:dryad-map,caption=\texttt{MapReduce} с использованием \texttt{DryadLINQ}]
public static MapReduce( 
  source, 
  mapper, // func(T) → Ms
  keySelector, // func(M) → K
  reducer // func(K,Ms) → Rs
  ) {
    var mapped = source.SelectMany(mapper);
    var groups = mapped.GroupBy(keySelector);
    return groups.SelectMany(reducer);
}

\end{lstlisting}
\end{center}

На рисунке~\ref{img:dryad_arch} изображнен план выполнения \texttt{DryadLINQ} задач.

\img{90mm}{dryad_arch}{План выполнения \texttt{DryadLINQ} задач.}

\subsubsection{Реализация}

Когда \texttt{DryadLINQ} получает управление, система начинает с преобразования необработанного выражения \texttt{LINQ} в граф плана выполнения (\texttt{EPG}), где каждый узел является оператором, а ребра представляют его входы и выходы. 
\texttt{EPG} тесно связан с традиционным планом запросов к базе данных, но используется более общая терминология плана выполнения, чтобы охватить вычисления, которые не просто сформулированы как <<запросы>>. 
\texttt{EPG} представляет собой ориентированный ациклический граф --- существование общих подвыражений и операторов, таких как \texttt{Fork}, означает, что \texttt{EPG} не всегда могут быть описаны деревьями. 
Затем \texttt{DryadLINQ} применяет оптимизацию для перезаписи терминов в \texttt{EPG}. 
\texttt{EPG} --- это <<скелет>> графа потока данных \texttt{Dryad}, который будет выполнен, и каждый узел \texttt{EPG} реплицируется во время выполнения для создания <<этапа>> \texttt{Dryad} (набора вершин, выполняющих одно и то же вычисление в разных разделах набора данных).

Статическая оптимизация \texttt{DryadLINQ} представляет собой правила условной перезаписи графа, запускаемые предикатом свойств узла \texttt{EPG}. 
Большинство статических оптимизаций сосредоточены на минимизации дискового и сетевого ввода-вывода. 
Наиболее важными являются:

\begin{enumerate}
  \item Конвейеризация: В одном процессе может быть выполнено несколько операторов. 
    Конвейерные процессы сами по себе являются выражениями \texttt{LINQ} и могут выполняться существующей реализацией \texttt{LINQ} на одном компьютере. 
  \item Устранение избыточности: \texttt{DryadLINQ} удаляет ненужные шаги разделения хэша или диапазона. 
  \item Быстрая агрегация: Поскольку повторное разбиение наборов данных обходится дорого, последующие агрегации перемещаются перед операторами разбиения там, где это возможно.
  \item Сокращение ввода-вывода: Там, где это возможно, \texttt{DryadLINQ} использует \texttt{TCP}-канал \texttt{Dryad} и каналы \texttt{FIFO} в памяти вместо сохранения временных данных в файлах. 
    Система по умолчанию сжимает данные перед выполнением разбиения на разделы, чтобы уменьшить сетевой трафик. 
    Пользователи могут вручную переопределить настройки сжатия, чтобы сбалансировать загрузку процессора с нагрузкой на сеть, если оптимизатор примет неверное решение.
\end{enumerate}

В качестве динамической оптимизации, \texttt{DryadLINQ} использует \texttt{Dryad API} для динамического изменения графика выполнения по мере того, как информация из запущенного задания становится доступной.
Агрегирование дает возможность для большого сокращения времени ввода-вывода, поскольку оно может быть оптимизировано в виде дерева в соответствии с местоположением, агрегируя данные сначала на уровне компьютера, затем на уровне ряда компьютеров и, наконец, на уровне всего кластера.

\texttt{EPG} используется для получения плана выполнения \texttt{Dryad} после фазы статической оптимизации. 
Хотя \texttt{EPG} кодирует всю необходимую информацию, это не запускаемая программа. 
\texttt{DryadLINQ} использует динамическую генерацию кода для автоматического синтеза кода \texttt{LINQ} для запуска в вершинах \texttt{Dryad}. 
Сгенерированный код компилируется в сборку \texttt{.NET}, которая отправляется на компьютеры кластера во время выполнения. 
Для каждого этапа планирования выполнения сборка содержит два фрагмента кода: 

\begin{enumerate}
  \item Код для подвыражения \texttt{LINQ}, выполняемого каждым узлом.
  \item Код сериализации данных канала. 
    Этот код намного эффективнее стандартных методов сериализации \texttt{.NET}, поскольку он может полагаться на контракт между считывателем и записывающим устройством канала для доступа к одному и тому же статически известному типу данных. 
\end{enumerate}

Подвыражение в вершине строится из фрагментов общего \texttt{EPG}, передаваемого в \texttt{DryadLINQ}. 
\texttt{EPG} создается в контексте выполнения исходного клиентского компьютера и может зависеть от этого контекста двумя способами:

\begin{enumerate}
  \item Выражение может ссылаться на переменные в локальном контексте. 
    Эти ссылки удаляются путем частичного вычисления подвыражения во время генерации кода. 
    Для примитивных значений ссылки в выражениях заменяются фактическими значениями. 
    Значения объектов сериализуются в файл ресурсов, который отправляется на компьютеры в кластере во время выполнения.
  \item Выражение может ссылаться \texttt{.NET}. 
    \texttt{.NET Reflection} используется для поиска транзитивного закрытия всех несистемных библиотек, на которые ссылается исполняемый файл, и они отправляются на компьютеры кластера во время выполнения.
\end{enumerate}

% \subsection{Google Dataflow и Apache Beam}
\subsection{Google Dataflow}

\subsubsection{Программная модель}

Термин <<Dataflow>> используется для описания модели обработки данных <<Google Cloud Dataflow>>~\cite{dataflow}, которая основана на технологии <<FlumeJava>>~\cite{flumejava} и <<MillWheel>>~\cite{millwheel}.
Одной из реализаций модели <<Dataflow>> является \texttt{Apache Beam}.

Модель \texttt{Dataflow} содержит две основных операций, которые работают с парами ключ-значение, проходящими через систему~\cite{dataflow-paper}:

\begin{itemize}
  \item \texttt{ParDo} для универсальной параллельной обработки. 
    Каждый обрабатываемый входной элемент (который сам по себе может быть конечной коллекцией) предоставляется определяемой пользователем функции (называемой \texttt{DoFn} в \texttt{Dataflow}), которая может выдавать ноль или более выходных элементов на вход. 
    Например, рассмотрим операцию, которая расширяет все префиксы ключа ввода, дублируя значение между ними:

    \[
    \begin{split}
      (f_{ix}, 1)&, (f_{it}, 2) \\
      &\Bigg\downarrow ParDo(ExpandPrefixes) \\
      (f, 1), (f_{i}, 1), (f_{ix}, 1)&, (f, 2), (f_{i}, 2), (f_{it}, 2) 
    \end{split}
  \]
  
\item \texttt{GroupByKey} для группируемых пар ключ-значение по ключу.
    \[
    \begin{split}
      (f, 1), (f_{i}, 1), (f_{ix}, 1)&, (f, 2), (f_{i}, 2), (f_{it}, 2) \\
      &\Bigg\downarrow GroupByKey \\
      (f, [1, 2]), (f_{i}, [1, 2])&, (f_{ix}, [1]), (f_{it}, [2]) 
    \end{split}
  \]
\end{itemize}

Операция \texttt{ParDo} работает поэлементно с каждым входным элементом и, таким образом, естественным образом преобразуется в работу с потоковыми данными. 
Операция \texttt{GroupByKey}, с другой стороны, собирает все данные для данного ключа перед отправкой их вниз по потоку. 
Если источник входных данных неограничен, у нас нет возможности узнать, когда он закончится. 
Распространенным решением этой проблемы является обработка данных окнами.

Системы, поддерживающие группировку данных, обычно переопределяют свою операцию \texttt{GroupByKey}, по сути, превращая ее в \texttt{GroupByKeyAndWindow}. 
Особенность \texttt{Dataflow} модели заключается в поддержке невыровненных окон, за которыми закреплены 2 основные идеи. 
Первое заключается в том, что проще рассматривать все стратегии управления окнами как невыровненные с точки зрения модели и позволять базовым реализациям применять оптимизации, относящиеся к выровненным случаям, где это применимо. 
Второе заключается в том, что управление окнами можно разбить на две взаимосвязанные операции:
\begin{itemize}
  \item \texttt{Set<Window> AssignWindows(T datum)}, который присваивает элементу ноль или более окон.
  \item \texttt{Set<Window> MergeWindows(Set<Window> windows)}, который объединяет окна во время группировки. 
    Это позволяет создавать окна по мере поступления данных и группировать их вместе. 
\end{itemize}

Следует обратить внимание, что для поддержки оконного управления временем события изначально, вместо передачи пар (ключ, значение) через систему, теперь передается (ключ, значение, время события, окно) 4-значный кортеж. 
Элементы предоставляются системе с временными метками времени события (которые также могут быть изменены в любой точке конвейера) и первоначально назначаются глобальному окну по умолчанию, охватывающему все время события, предоставляя семантику, соответствующую значениям по умолчанию в стандартной пакетной модели.
  \[
    \begin{split}
      (k, v1, 12:00, [0, ∞))&, (k, v2, 12:01, [0, ∞) \\
      &\Bigg\downarrow AssignWindows(Sliding(2m, 1m)) \\
      (k, v1, 12:00,\ &[11:59, 12:01)), \\ 
      (k, v1, 12:00,\ &[12:00, 12:02)), \\
      (k, v2, 12:01,\ &[12:00, 12:02)), \\
      (k, v2, 12:01,\ &[12:01, 12:03))
    \end{split}
  \]

\subsubsection{Реализация}

Реализация \texttt{Google Cloud Dataflow} во многом основнана на других проектах \texttt{Google} -- \texttt{FlumeJava}~\cite{flumejava}, в качестве модели для пакетной обработки данных, и \texttt{MillWheel}, в качестве модели для потоковой обработки данных.

Центральным классом библиотеки \texttt{FlumeJava} --- \texttt{PCollection<T>}, зачастую большой неизменяемый набор элементов типа \texttt{T}. Коллекция может иметь либо четко определенный порядок (называемый последовательностью), либо элементы могут быть неупорядоченными (называемыми коллекцией). Поскольку они менее ограничены, коллекции более эффективны для создания и обработки, чем последовательности. \texttt{PCollection<T>} может быть создана из находящейся в памяти \texttt{Java PCollection<T>}. \texttt{PCollection<T>} также может быть создана путем чтения файла в одном из нескольких возможных форматов. Например, текстовый файл может быть прочитан как \texttt{PCollection<String>}, а файл с двоичным представлением, может быть прочитан как \texttt{PCollection<T>}, учитывая спецификацию того, как декодировать каждую двоичную запись в объект \texttt{Java} типа \texttt{T}. Могут быть прочитаны наборы данных, представленные несколькими фрагментами файла в виде единой логической коллекции.

Вторым основным классом \texttt{FlumeJava} является \texttt{PTable<K,V>}, который представляет собой неизменяемую мультикарту с ключами типа \texttt{K} и значениями типа \texttt{V}. 

\texttt{PTable<K,V>} является подклассом \texttt{PCollection<Pair<K,V>>} и действительно представляет собой просто неупорядоченный набор пар. Некоторые операции \texttt{FlumeJava} применяются только к коллекциям пар, и в контексте \texttt{Java} было сделано решение определить подкласс, чтобы охватить эту абстракцию; на другом языке \texttt{PTable<K,V>} лучше было бы определить как синоним типа \texttt{PCollection<Pair<K,V>>}.

Чтобы обеспечить оптимизацию запросов \texttt{FlumeJava}, параллельные операции  выполняются лениво с использованием отложенного вычисления. 
Каждый объект коллекции представлен внутри либо в отложенном (еще не вычисленном), либо в материализованном (вычисленном) состоянии. 
Отложенная коллекция содержит указатель на отложенную операцию, которая ее вычисляет. 
Отложенная операция, в свою очередь, содержит ссылки на коллекции, которые являются ее аргументами (которые сами по себе могут быть отложены или материализованы), и отложенные коллекции, которые являются ее результатами. Когда вызывается операция FlumeJava, она просто создает объект отложенной операции и возвращает новую отложенную коллекцию, которая указывает на нее. 
Таким образом, результатом выполнения серии операций \texttt{FlumeJava} является направленный ациклический граф отложенных коллекций и операций --- граф плана выполнения

На высоком уровне \texttt{MillWheel} представляет собой граф определяемых пользователем преобразований входных данных, которые генерируют выходные данные. Каждое из этих преобразований может быть распараллелено на произвольном количестве машин, так что пользователю не нужно беспокоиться о балансировке нагрузки на детальном уровне. Абстрактно входные и выходные данные в \texttt{MillWheel} представлены тройками (ключ, значение, временная метка). 
В то время как ключ является полем метаданных, имеющим семантическое значение в системе, значением может быть произвольная строка байтов, соответствующая всей записи. 
Контекст, в котором выполняется пользовательский код, ограничен определенным ключом, и каждое вычисление может определять ключ для каждого источника ввода в зависимости от его логических потребностей. 
Временным меткам в этих тройках пользователь \texttt{MillWheel} может присвоить произвольное значение (но обычно они близки к времени системных часов, когда произошло событие). Если бы пользователь агрегировал количество поисковых запросов в секунду, то он хотел бы присвоить значение временной метки, соответствующее времени, в которое был выполнен поиск.

В совокупности конвейер пользовательских вычислений формирует граф потока данных, поскольку выходные данные одного из вычислений становятся входными данными для другого и так далее.
Отложенная операция, в свою очередь, содержит ссылки на коллекции, которые являются ее аргументами (которые сами по себе могут быть отложены или материализованы), и отложенные коллекции, которые являются ее результатами. Когда вызывается операция FlumeJava, она просто создает объект отложенной операции и возвращает новую отложенную коллекцию, которая указывает на нее. 
Таким образом, результатом выполнения серии операций \texttt{FlumeJava} является направленный ациклический граф отложенных коллекций и операций --- граф плана выполнения

На высоком уровне \texttt{MillWheel} представляет собой граф определяемых пользователем преобразований входных данных, которые генерируют выходные данные. Каждое из этих преобразований может быть распараллелено на произвольном количестве машин, так что пользователю не нужно беспокоиться о балансировке нагрузки на детальном уровне. Абстрактно входные и выходные данные в \texttt{MillWheel} представлены тройками (ключ, значение, временная метка). 
В то время как ключ является полем метаданных, имеющим семантическое значение в системе, значением может быть произвольная строка байтов, соответствующая всей записи. 
Контекст, в котором выполняется пользовательский код, ограничен определенным ключом, и каждое вычисление может определять ключ для каждого источника ввода в зависимости от его логических потребностей. 
Временным меткам в этих тройках пользователь \texttt{MillWheel} может присвоить произвольное значение (но обычно они близки к времени системных часов, когда произошло событие). Если бы пользователь агрегировал количество поисковых запросов в секунду, то он хотел бы присвоить значение временной метки, соответствующее времени, в которое был выполнен поиск.

В совокупности конвейер пользовательских вычислений формирует граф потока данных, поскольку выходные данные одного из вычислений становятся входными данными для другого и так далее.
Пользователи могут добавлять и удалять вычисления из топологии динамически, без необходимости перезагрузки всей системы. 
При манипулировании данными и выводе записей вычисление может произвольно комбинировать, изменять, создавать и удалять записи.

\subsection{Apache Flink}

\subsubsection{Програмная модель}

\texttt{Apache Flink} --- это система с открытым исходным кодом для обработки потоковых и пакетных данных.


\texttt{Apache Flink} следует парадигме, которая включает обработку потоков данных в качестве объединяющей модели для анализа в реальном времени, непрерывных потоков и пакетной обработки как в модели программирования, так и в механизме выполнения.
В сочетании с устойчивыми очередями сообщений, которые позволяют квази-произвольное воспроизведение потоков данных (как в \texttt{Apache Kafka} или \texttt{Amazon Kinesis}), программы потоковой обработки не делают различий между обработкой последних событий в режиме реального времени, непрерывной периодической агрегацией данных в больших окнах или обработкой терабайт информации.
Вместо этого эти различные типы вычислений просто начинают свою обработку в разных точках долговременного потока и поддерживают разные формы состояния во время вычислений. 
Благодаря очень гибкому оконному механизму программы \texttt{Flink} могут вычислять как ранние, так и приблизительные, а также отложенные и точные результаты за одну и ту же операцию, устраняя необходимость комбинировать различные системы для двух вариантов использования. 
\texttt{Flink} поддерживает различные понятия времени (время события, время приема, время обработки), чтобы предоставить программистам высокую гибкость в определении того, как события должны быть соотнесены.

На рисунке~\ref{img:flink_model} изображена програмная модель \texttt{Apache Flink}.

\img{70mm}{flink_model}{Программная модель \texttt{Flink}}

Промежуточные потоки данных \texttt{Flink} являются основной абстракцией для обмена данными между операторами. 
Промежуточный поток данных представляет собой логический дескриптор данных, которые создаются оператором и могут использоваться одним или несколькими операторами.
Промежуточные потоки логичны в том смысле, что данные, на которые они указывают, могут материализоваться на диске, а могут и не материализоваться. 
Конкретное поведение потока данных параметризуется более высокими уровнями \texttt{Flink} (например, программным оптимизатором, используемым \texttt{API} набора данных).

\begin{enumerate}
  \item Конвейерный и блокирующий обмен данными.

    Это позволяет \texttt{Flink} достигать высокой пропускной способности, устанавливая размер буферов на высокое значение (например, несколько килобайт), а также низкой задержки, устанавливая время ожидания буфера на низкое значение (например, несколько миллисекунд).
  \item Управляющие события.

    Помимо обмена данными, потоки в \texttt{Flink} передают разные типы управляющих событий. 
    Это специальные события, вводимые в поток данных операторами, и доставляются по порядку вместе со всеми другими записями данных и событиями в разделе потока. 
    Принимающие операторы реагируют на эти события, выполняя определенные действия по их прибытии. 
    \texttt{Flink} использует множество специальных типов контрольных событий, включая:
    \begin{itemize}
      \item барьеры контрольных точек, которые координируют контрольные точки, разделяя поток на до-контрольные точки и после-контрольные точки.
      \item <<водяные знаки>>, сигнализирующие о ходе событий во времени внутри раздела потока;
      \item итерационные барьеры, сигнализирующие о том, что раздел потока достиг конца суперэтапа.
    \end{itemize}
\end{enumerate}

\subsubsection{Реализация}

\texttt{Flink} предоставляет низкоуровневую операцию потоковой обработки -- \texttt{ProcessFunction} -- которая обеспечивает доступ к основным компоновочным блокам любого потокового приложения~\cite{flink-oreilly}:
\begin{enumerate}
  \item События (отдельные записи в потоке);
  \item Состояние (отказоустойчивое, согласованное);
  \item Таймеры (время события и время обработки).
\end{enumerate}

На рисунке~\ref{img:flink_arch} изображена итерационная можель обработки потоков в \texttt{Apache Flink}.
\img{70mm}{flink_arch}{Итерационная модель \texttt{Flink}}

Реализация операций над двумя входными потоками обеспечивается с помощью низкоуровневой \texttt{Flink} операции \texttt{join}, которая привязана к двум разным входам (при необходимости объединить более двух потоков, можно каскадировать несколько низкоуровневых объединений) и предоставляет индивидуальные методы обработки записей из каждого входного сигнала. 
Реализация низкоуровневого соединения обычно выполняется по следующей схеме:
\begin{enumerate}
  \item Создание и поддержка объектов состояния, отражающих текущее состояние выполнения.
  \item Обновление состояний при получении элементов с одного (или обоих) входных потока.
  \item Использование текущего состояния для преобразования данных и получения результата при получении элементов с одного или обоих входных потока.
\end{enumerate}

Кластер Flink включает в себя три типа процессов: клиент, менеджер работ (\texttt{JobManager} и, по крайней мере, один менеджер задач (\texttt{TaskManager}). Клиент берет программный код, преобразует его в граф потока данных и отправляет его в \texttt{JobManager}. На этом этапе преобразования также проверяются типы данных (схемы) данных, которыми обмениваются операторы, и создаются сериализаторы и другой код, специфичный для типа/схемы. 

\texttt{Dataflow} программы дополнительно проходят этап оптимизации запросов на основе затрат, аналогичный физической оптимизации, выполняемой оптимизаторами реляционных запросов.
\texttt{JobManager} координирует распределенное выполнение потока данных. 
Он отслеживает состояние и прогресс каждого оператора и потока, планирует новых операторов и координирует контрольные точки и восстановление.
При настройке высокой доступности \texttt{JobManager} сохраняет минимальный набор метаданных на каждой контрольной точке в отказоустойчивом хранилище, так что резервный \texttt{JobManager} может восстановить контрольную точку и восстановить выполнение потока данных оттуда. 
Фактическая обработка данных происходит в \texttt{TaskManager}.
\texttt{TaskManager} выполняет один или несколько операторов, которые создают потоки, и сообщает об их статусе \texttt{TaskManager}. 
\texttt{TaskManager} поддерживают буферные пулы для буферизации или материализации потоков и сетевые подключения для обмена потоками данных между операторами.
Он отслеживает состояние и прогресс каждого оператора и потока, планирует новых операторов и координирует контрольные точки и восстановление.
При настройке высокой доступности \texttt{JobManager} сохраняет минимальный набор метаданных на каждой контрольной точке в отказоустойчивом хранилище, так что резервный \texttt{JobManager} может восстановить контрольную точку и восстановить выполнение потока данных оттуда. 
Фактическая обработка данных происходит в \texttt{TaskManager}.
\texttt{TaskManager} выполняет один или несколько операторов, которые создают потоки, и сообщает об их статусе \texttt{TaskManager}. 
\texttt{TaskManager} поддерживают буферные пулы для буферизации или материализации потоков и сетевые подключения для обмена потоками данных между операторами.

\subsection{Apache Samza}

\subsubsection{Програмная модель}

\texttt{Apache Samza} --- это распределенная система для отказоустойчивой потоковой обработки с отслеживанием состояния. 

\texttt{Samza} использует секционированное локальное состояние наряду с механизмом фонового журнала изменений с низкими накладными расходами, что позволяет масштабировать его до сотен терабайт для каждого приложения. 
Восстановление после сбоев ускоряется за счет перепланирования на основе привязки к хосту. 
В дополнение к обработке бесконечных потоков событий, \texttt{Samza} поддерживает обработку конечного набора данных в виде потока либо из источника потоковой передачи, таких как \texttt{Kafka}, моментального снимка базы данных, либо из файловой системы (например \texttt{HDFS}), без необходимости изменять код приложения. 
Этим он отличается от архитектур на основе лямбда, которые требуют обслуживания отдельных баз кода для пакетной и потоковой обработки~\cite{10.14778/3137765.3137770}.

Задача в \texttt{Samza} состоит из набора экземпляров виртуальной машины \texttt{Java} (\texttt{JVM}), каждый из которых обрабатывает подмножество входных данных. 
Код, выполняемый в каждой \texttt{JVM}, включает в себя платформу \texttt{Samza} и пользовательский код, реализующий требуемую функциональность для конкретного приложения. 
Основным интерфейсом для пользовательского кода является \texttt{Java}-интерфейс \texttt{StreamTask}, который определяет метод \texttt{process()}. 
Как только \texttt{Samza}-задача развернута и инициализирована, фреймворк вызывает метод \texttt{process()} один раз для каждого сообщения в любом из входных потоков. 
Выполнение этого метода может иметь различные эффекты, включая запрос или обновление локального состояния и отправку сообщений в выходные потоки. 
Эта модель вычислений очень похожа на функцю \texttt{Map} модели \texttt{MapReduce} с той разницей, что входные данные \texttt{Samza}-задача обычно бесконечны (неограниченны). 
Аналогично \texttt{MapReduce}, каждая задача \texttt{Samza} представляет собой однопоточный процесс, который выполняет итерацию по последовательности входных записей. 
Входные данные для задания \texttt{Samza} разбиваются на непересекающиеся подмножества, и каждая входная секция назначается ровно одной задаче обработки. 
Одной и той же задаче обработки может быть назначено более одного раздела, и в этом случае обработка этих разделов чередуется в потоке задачи. 
Однако количество разделов во входных данных определяет максимальную степень параллелизма задач~\cite{Kleppmann2018}.

Интерфейс журнала предполагает, что каждый раздел входных данных представляет собой полностью упорядоченную последовательность записей и что каждая запись связана с монотонно увеличивающимся порядковым номером или идентификатором (известным как смещение). 
Поскольку записи в каждом разделе считываются последовательно, задача может отслеживать ход выполнения, периодически записывая смещение последней прочитанной записи в долговременное хранилище. 
Если задача потоковой обработки перезапускается, она возобновляет использование входных данных из последнего записанного смещения. 
Чаще всего \texttt{Samza} используется в сочетании с \texttt{Apache Kafka}

\texttt{Kafka} предоставляет секционированный отказоустойчивый журнал, который позволяет издателям добавлять сообщения в раздел журнала, а потребителям (подписчикам) для последовательного чтения сообщений в разделе журнала.
\texttt{Kafka} также позволяет заданиям потоковой обработки повторно обрабатывать ранее просмотренные записи, сбрасывая смещение потребителя на более раннюю позицию, что полезно при восстановлении после сбоев.

В то время как каждый раздел входного потока назначен одной конкретной задаче задания \texttt{Samza}, выходные разделы не привязаны к задачам. 
То есть, когда система отправляет выходные сообщения, он может назначить их любому разделу выходного потока. 
Этот факт может быть использован для группировки связанных элементов данных в тот же раздел, как показано на рисунке~\ref{img:samza-one-job}.

\img{50mm}{samza-one-job}{Пример исполнения задачи в системе \texttt{Samza}}

Когда потоковые задачи объединяются в многоступенчатые конвейеры обработки, выходные данные одной задачи становятся входными данными для другой задачи. 
В отличие от многих других фреймворков потоковой обработки, \texttt{Samza} не реализует свой собственный транспортный уровень сообщений для доставки сообщений между потоковыми операторами. 
Вместо этого для этой цели используется \texttt{Kafka}; поскольку \texttt{Kafka} записывает все сообщения на диск, он обеспечивает большой буфер между этапами конвейера обработки, ограниченный только доступным дисковым пространством на брокерах \texttt{Kafka}.

\subsubsection{Реализация}

\texttt{Samza}-задача --- это неделимый этап вычислений: в задачу подается один или несколько входных потоков; на входных данных выполняются различные обработки -- от простых операций (например, фильтрации, объединения и агрегации) до сложных алгоритмов машинного обучения; и генерируется один или несколько новых выходных потоков.

\texttt{Samza} представляет задачи в виде ориентированного графа операторов (вершин), соединенных потоками данных (ребрами).

Оператор --- это преобразование одного или многих потоков в другой поток (потоки). В зависимости от количества входных и выходных потоков \texttt{Samza} поддерживает три типа операторов.

\begin{enumerate}
  \item 1:1 операторы:
    \begin{itemize}
      \item $map$ -- применение определенной функции к каждому сообщению; 
      \item $filter$ -- фильтрация сообщений на основе функции;
      \item $window$ -- разбивает поток на окна и агрегирует;
      \item $partition$ -- перераспределение потока по другому ключу.
    \end{itemize}
  \item Много:1 операторы:
    \begin{itemize}
      \item $join$ -- объединение $\geq 2$ потоков в один поток на основе заданной функции;
      \item $merge$ -- слияние $\geq 2$ двух потоков в один поток.
    \end{itemize}
    \clearpage
  \item Определяемые пользователем -- разделение или репликация потока на $\geq 2$ потока. Это достигается за счет того, что несколько операторов могут использовать один и тот же поток.
\end{enumerate}

Внутренне, как показано на рисунке~\ref{img:samza-job}, задание делится на множество параллельных, независимых и идентичных задач, а входной поток делится на разделы (например, {P1, ..., Pp}). 
\img{120mm}{samza-job}{Внутренняя архитектура \texttt{Samza}-задачи}

Каждая задача выполняет идентичную логику, но на своем собственном входном разделе (подход параллелизма данных). 
Каждая задача запускает весь граф операторов. 
Для каждого входящего сообщения задача пропускает сообщение через граф (выполняя операторы над сообщением) до тех пор, пока не будет достигнут оператор без выходных данных или конечный поток выходных данных. 
Большинство ребер промежуточного потока остаются локальными для задачи, т. е. они не пересекают границу задачи. 
Это сохраняет большинство коммуникаций локальными и минимизирует сетевой ввод-вывод. 
Единственным исключением является оператор разделения, где сообщения перераспределяются между всеми задачами на основе логики разделения. 
Для нелокальных потоков и входных и выходных потоков задач \texttt{Samza} использует отказоустойчивый (без потери сообщений) и воспроизводимый (с большими возможностями буферизации) механизм связи.

\subsection*{Вывод}

В данном разделе были рассмотрены основные подходы к обработке данных. 
Были рассмотрены основные принципы работы пакетных и потоковых систем, а также рассмотрены основные реализации таких систем. 
Были рассмотрены системы семейства \texttt{Hadoop}, \texttt{Spark}, \texttt{Pregel}, \texttt{DryadLINQ}, \texttt{Google Dataflow}, \texttt{Apache Flink} и \texttt{Apache Samza}. 
Все рассмотренные системы обладают своими преимуществами и недостатками, и могут быть применены в зависимости от поставленной задачи. 

\clearpage
